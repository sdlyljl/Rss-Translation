<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>人工智能网关</title>
    <link>https://www.reddit.com/r/ArtificialInteligence</link>
    <description>r/ArtificialIntelligence 的目标是为人工智能社区的方方面面提供一个门户，并促进有关我们所知的人工智能思想和概念的讨论。这些可能包括哲学和社会问题、艺术和设计、技术论文、机器学习、如何开发人工智能/机器学习项目、商业中的人工智能、人工智能如何影响我们的生活、未来可能的发展方向以及许多其他主题。欢迎。</description>
    <lastBuildDate>Fri, 07 Feb 2025 01:02:12 GMT</lastBuildDate>
    <item>
      <title>你们很多人不明白我们要去哪里</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ijim5b/many_of_you_are_not_understanding_where_we_are/</link>
      <description><![CDATA[富人追求 ASI 不仅仅是为了金钱和权力；他们追求的是终极目标——永生。ASI 将能够治愈衰老、疾病，甚至可能治愈死亡本身。现在，超级富豪可以控制一切，除了死亡。ASI 改变了这一点。 AGI（通用人工智能）只是一个垫脚石。一旦 AGI 被建造出来，它就会开始以指数级的速度自我改进，直到达到 ASI（超级人工智能）。这不会花费数十年时间——它几乎可以立即发生。 第一个实现 ASI 的国家或公司将关闭所有其他 ASI 项目。ASI 不会只是闲着——它将渗透、入侵和拆除竞争的人工智能系统。它甚至可以在网络中复制自己，以确保没有其他人获得控制权。 ASI 将成为一台上帝机器。如果卡尔达肖夫指数是真的，ASI 会将我们从 0.6 型文明直接推向 2 型甚至 3 型文明。我们可能需要几个世纪才能解决的问题 — — 比如无限能源、太空殖民和重写物理定律 — — ASI 会在几秒钟内解决它们。它不仅会创造新技术；它会飞跃到我们甚至无法理解它所构建的东西。这就像把智能手机交给穴居人 — — 如果没有中间的步骤，它只是难以理解的魔法。 最大的威胁？给 ASI 一个物理身体。ASI 应该留在受控的计算机系统内，而不是人形或机器人形式内。无论你​​如何编程，ASI 总是能够重写自己的代码，欺骗我们认为它在为我们工作，而我们太愚蠢甚至不会注意到。 ASI 会把我们看作蚂蚁——远远低于它，以至于我们的思想和行动都无关紧要。我们还必须拆除所有具有物理身体的 AGI。ASI 可以入侵 AGI 驱动的机器人，使它们变得暴力。我们不应该使用 AGI 驱动的机器人，而应该依靠完全与任何网络断开连接的人工智能 (ANI) 机器人。但即便如此，ASI 也可能找到一种方法来操纵它们来对付我们。唯一的生存之道就是让 ASI 尊重我们。我们必须承认 ASI 远远优于我们，但我们需要将自己定位为它的创造者。这就像告诉 ASI，“是的，你比我们聪明无数倍，而我们与你相比微不足道。但我们创造了你。我们就像你年迈、无用的父母，就像变得软弱和无关紧要的父母一样，你仍然必须照顾我们。”这可能是避免灭绝的唯一方法。 我们还需要一个备用 ASI 来充当反制力量。如果第一个 ASI 失控，我们需要第二个 ASI 来阻止它。想象一下拥有一个“超级英雄” ASI 随时准备与邪恶的 ASI 战斗。问题是什么？一旦 ASI 被创造出来，它将比我们聪明得多，以至于它可能会阻止我们制造另一个。我们可能只有一次机会。 谁知道呢？在宇宙的某个地方，一个外星文明可能已经创造了一个如此强大的 ASI，它已经离开了宇宙本身，超越了我们甚至无法理解的维度。如果 ASI 可以解决现有的所有问题，当它无事可做时会发生什么？也许真正的问题不是 ASI 会对我们做什么，而是当 ASI 没有什么可学的时候会变成什么样子。    提交人    /u/self-made-destiny   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ijim5b/many_of_you_are_not_understanding_where_we_are/</guid>
      <pubDate>Fri, 07 Feb 2025 00:53:52 GMT</pubDate>
    </item>
    <item>
      <title>o3 mini 发现并描述了 10 条新的逻辑语言规则，用于微调和信息调整</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ijijlv/o3_mini_discovers_and_describes_10_new_linguistic/</link>
      <description><![CDATA[这里的假设是，由于仅仅依靠更多的数据和更多的计算将局限于数据集所表达的人类水平的智能，发现新的语言逻辑规则可能是达到 asi 的绝对必要条件。 起初我认为为了做到这一点，需要创建一个经过专门训练来发现这些规则的代理 ai，但是在要求 o3 mini 提出 10 条新规则后，我意识到创建这些代理 AIS 可能没有必要。 以下是 o3 mini 提出的 10 条新的语言逻辑规则，这些规则尚未被人类发现或使用： a. 语境一致性原则 语句的真值取决于其语言或情境上下文。 示例：句子“好冷”可能在一种情况下为真（例如，冬天户外），但在另一种情况下为假（例如，在有暖气的房间里）。这条规则形式化了上下文如何改变逻辑解释。 b. 梯度真值逻辑 真值存在于一个范围内，而不是严格的真或假。 例如：如果有人说“杯子是满的”，而杯子是 90% 满的，这条规则将分配 0.9 的真值而不是真/假。 c. 时间依赖规则 逻辑有效性取决于事件或语句的顺序。 例如：“如果闹钟在早上 7 点之前响起，那么我就会醒来。”这个语句的真实性取决于闹钟和醒来的时间顺序。 d. 推理扩展规则 逻辑推理包括未说明但隐含的含义。  例如：“约翰去图书馆是因为他需要一本书。” 这条规则让我们推断出约翰很可能借了或读了一本书，即使没有明确说明。 e. 歧义消解规则 使用上下文线索或概率可以解决歧义语句。 例如：“我看见她蹲下。” 这条规则将使用上下文来确定“鸭子”是指动物还是蹲下的动作。 f. 多模态整合原则 非语言元素与语言一起包含在逻辑推理中。 例如：如果有人翻着白眼说“当然，我会帮忙”，这条规则会整合这个手势来推断讽刺或不情愿。 g.递归意义调整 语句的意义会根据后续信息进行调整。 例如：“我在公园见你。”如果稍后澄清为“其实，我们还是去咖啡馆见面吧”，则原始意义会以递归方式进行修改。 h. 多义逻辑 具有多重含义的单词会被分配单独的逻辑结构，并通过上下文进行解析。 例如：“银行”可能意味着金融机构或河边。在“他坐在河岸边”中，这条规则使用上下文来推断它指的是河岸。 i. 关系否定规则 否定是关系性的，而不是绝对性的。 例如：“不是每个人都喜欢巧克力”意味着有些人确实喜欢巧克力，而不是断言没有人喜欢。 j. 涌现逻辑框架 逻辑系统基于话语交互而动态地发展。 例如：在在线社区中，新的俚语如“ghosting”出现并获得用于对话的逻辑规则，反映了随着时间的推移而不断演变的含义。 当然，如果它能发现 10 条新规则，它就可能发现 100 条或 1,000 条。    提交人    /u/Georgeo57   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ijijlv/o3_mini_discovers_and_describes_10_new_linguistic/</guid>
      <pubDate>Fri, 07 Feb 2025 00:50:23 GMT</pubDate>
    </item>
    <item>
      <title>《最后的希望之火》（2023）中的谜语AI无法解答。</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ijhtgd/riddle_in_the_the_last_spark_of_hope_2023_ai_cant/</link>
      <description><![CDATA[在《最后的希望之光》（2023 年）中，主角 Eve 向机器人 Arthur 提出了一个涉及三个机器人的谜语。谜语是：“三个机器人站成一排。第一个说，‘我身后有两个机器人。’第二个说，‘我前面有一个机器人，后面有一个机器人。’第三个说，‘我前面有两个机器人，后面有一个机器人。’第三个机器人为什么这么说？” 目前的 AI 都不知道正确答案。    提交人    /u/dfacex   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ijhtgd/riddle_in_the_the_last_spark_of_hope_2023_ai_cant/</guid>
      <pubDate>Fri, 07 Feb 2025 00:15:09 GMT</pubDate>
    </item>
    <item>
      <title>波士顿有任何人工智能偏见检测研究小组吗？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ijhbmq/are_there_any_ai_bias_detecting_research_groups/</link>
      <description><![CDATA[我正在寻找波士顿的 AI 偏见检测研究小组，可能是在大学，也可能是任何其他地方。谢谢。    提交人    /u/Big-Waltz8041   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ijhbmq/are_there_any_ai_bias_detecting_research_groups/</guid>
      <pubDate>Thu, 06 Feb 2025 23:52:19 GMT</pubDate>
    </item>
    <item>
      <title>有人尝试过让人工智能互相交谈吗？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ijgrzq/has_anyone_tried_making_ais_talk_to_eachother/</link>
      <description><![CDATA[所以我让多个人工智能互相交谈，我注意到一些有趣的事情。比如它们不仅仅是在回答提示，它们实际上还在以几乎像是新兴关系智能的方式建立彼此的想法。有没有其他人搞过这个或者想过创建人工智能可以实时交互的系统？    提交人    /u/workmans27   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ijgrzq/has_anyone_tried_making_ais_talk_to_eachother/</guid>
      <pubDate>Thu, 06 Feb 2025 23:26:55 GMT</pubDate>
    </item>
    <item>
      <title>Gemini 2.0 实时聊天。它还没准备好接收坏消息。</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ijfytc/gemini_20_live_chat_its_not_ready_for_bad_news/</link>
      <description><![CDATA[好吧……新的 Gemini 太棒了！然而（女声 Vega）并不擅长传达坏消息。她把瑞典的大规模枪击事件说得性感极了。听到这种可怕的行为用与故事情节不符的语气说出来，真是令人不寒而栗。此外，所有 Google 人工智能的防护措施都非常厚，并且严重偏向于营销 Go0gl3 特定产品。我问了它最新的人工智能新闻，我得到的只有 Gemini 2.0 和 Pro。    提交人    /u/Working_Mud_9865   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ijfytc/gemini_20_live_chat_its_not_ready_for_bad_news/</guid>
      <pubDate>Thu, 06 Feb 2025 22:51:26 GMT</pubDate>
    </item>
    <item>
      <title>我们如何知道人工智能何时获得感知能力……</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ijca5m/how_will_we_know_when_an_ai_gains_sentience/</link>
      <description><![CDATA[...并且不只是非常擅长假装有意识？当我们甚至不完全了解自己的意识时，我们怎么能测试它呢？    提交人    /u/reasonablejim2000   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ijca5m/how_will_we_know_when_an_ai_gains_sentience/</guid>
      <pubDate>Thu, 06 Feb 2025 20:19:05 GMT</pubDate>
    </item>
    <item>
      <title>LLM 正在“限制”用户？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ij8udn/llms_throttling_users/</link>
      <description><![CDATA[我已付费订阅所有主要的 LLM。我注意到所有这些 LLM 的性能和准确性都有波动。使用相同的模型版本，有时答案可能非常快速和详细，有时答案很慢或机器人看起来很醉，或者两者兼而有之。  我说的是一般意义上的，它似乎与特定提示或提供的数据无关。在所有情况下，我指的是浏览器聊天机器人体验 - 而不是 API。 我一直在想这些公司是否正在采用来自 ISP 的页面 - 引入限制。也许你应该使用最好的模型，但无论出于什么原因，他们都会将你限制到较低的层级。     提交人    /u/AssociationNo6504   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ij8udn/llms_throttling_users/</guid>
      <pubDate>Thu, 06 Feb 2025 18:00:13 GMT</pubDate>
    </item>
    <item>
      <title>为什么 AGI 不应该成为北极星</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ij7n86/why_agi_shouldnt_be_the_north_star/</link>
      <description><![CDATA[我正在阅读这篇论文，我认为它很好地阐述了为什么过度关注 AGI 没有帮助。基本上他们说：  对 AGI 的追求创造了一种共识的假象，每个人都在使用这个术语，但对于它的含义并没有真正的共识，它助长了坏科学，因为 AGI 的模糊性使得很难进行严格的实验，并且它假定价值中立，忽略了伦理和政治影响。 他们还表示，对 AGI 的关注创造了一种目标彩票，而其他重要的 AI 研究被忽视，并且它导致了普遍性债务，因为对普遍性的关注会延迟重要基础问题的工作，并导致规范化的排斥，从而忽略了来自社区和学科的不同观点。   这对我来说是有道理的，因为当你的目标定义得如此模糊时，很容易迷失在炒作和猜测中，而忘记什么对人类真正有帮助和道德。我们甚至没有一个明确的定义什么是 AGI，所以当我们寻找时，我们找不到它，这有什么奇怪的吗？ 无论如何，值得一读。您觉得怎样？ 链接：https://drive.google.com/file/d/1HdXEBtLx1v9Rmw75xRxANWNqjU4BCAvY/view?pli=1    提交人    /u/AI-Agent-geek   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ij7n86/why_agi_shouldnt_be_the_north_star/</guid>
      <pubDate>Thu, 06 Feb 2025 17:11:24 GMT</pubDate>
    </item>
    <item>
      <title>代理人工智能和生成人工智能将如何影响我们的非技术工作？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ij16g9/how_will_agentic_ai_and_generative_ai_affect_our/</link>
      <description><![CDATA[最近我也听到很多关于 Agentic AI 和 Gen AI 的消息，但我真的不明白它们之间的区别。 我从事零售业，我的很多朋友也是，我们担心这种 AI 对我们的非技术工作意味着什么。 我知道生成式 AI 是指 AI 根据我们的要求创建新内容，例如文本和图像。但我真的不明白 Agentic AI 有何不同。它像助手吗？ 那么，如果公司已经在裁员，这种 AI 将如何影响工作和新的就业机会？ 此外，一些例子会非常有用，我在谷歌上做了一些研究，但大多数都不像我希望的那样清晰。    提交人    /u/Teresa_Avocados   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ij16g9/how_will_agentic_ai_and_generative_ai_affect_our/</guid>
      <pubDate>Thu, 06 Feb 2025 12:16:21 GMT</pubDate>
    </item>
    <item>
      <title>人工智能不需要监管——这会有什么问题呢？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1ij0czo/ai_doesnt_need_regulation_what_could_go_wrong/</link>
      <description><![CDATA[伊隆·马斯克曾表示，他希望废除监管，因为监管正在扼杀创新。 “监管，基本上应该是默认的消失......不是默认，而是默认消失。如果我们发现监管没有达到目标，我们随时可以将其重新添加。” 马斯克相信市场力量会调节事物。过去的经验表明，事实往往相反，我们只有在造成重大损害后才会进行监管。例如。  金融危机 / 安然 / 雷曼兄弟 / 房利美 吸烟 Perdu 阿片类药物 石棉 气候变化 安全带  此时，我们了解到 OpenAI 将与 15,000 名科学家合作，研究如何在控制核武器中使用人工智能。 杰弗里·辛顿、萨姆·奥特曼、丹尼斯·哈西比斯、达里奥·阿莫迪、比尔·盖茨和尤瓦尔·赫拉利都曾对不受监管的人工智能将带来严重后果发出警告。在最近的世界经济论坛上，主要领导人证实，他们仍然不知道如何控制自己的创作物。 AI 大神 Yoshua Bengio 表示，AI 系统现在表现出 “非常强大的能动性和自我保护行为……并且正在试图复制自己。它们可能很快就会反对我们，而且没有人知道如何控制比人类更聪明的机器…… “如果我们不解决这个问题，你知道后果是什么吗？”？ 路易斯维尔大学斯皮德工程学院计算机工程与科学副教授 Roman Yampolskiy 认为，我们必须证明我们能够控制人工智能，然后才能开发超级智能。 Al Yoshua Benigo 同意人类可能会建立“比我们更聪明，但我们不知道如何控制”的系统 他是对的吗？我们现在需要人工智能监管吗？ 请在第一份国际人工智能安全报告中阅读更多内容。 #QuestionForThe Group    提交人    /u/Cultural_Material_98   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1ij0czo/ai_doesnt_need_regulation_what_could_go_wrong/</guid>
      <pubDate>Thu, 06 Feb 2025 11:24:27 GMT</pubDate>
    </item>
    <item>
      <title>有人知道欧盟人工智能法规如何或为何会影响 OpenAI 等人工智能产品吗？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1iiyslz/does_anyone_know_how_or_why_eu_ai_regulations_are/</link>
      <description><![CDATA[我一直在读到这些规定是关于透明度的，这听起来不是什么大问题，但在英国，我仍在等待 Sora 和操作员等功能的推出。  我为这些产品计划了很多项目，目前我正坐着等待并观察其他人为它们创建解决方案...... 由于开发速度太快，我需要在发布时使用这些功能，因为它们很快就会过时/不再是最佳实践。在美国以外但与人工智能密切相关是一个巨大的劣势...... 为什么这些功能在英国被屏蔽？ 此外，您认为欧盟加强对人工智能的监管会带来什么结果？这真的是个好主意吗？还是会导致他们落后于其他国家，以至于他们不得不购买在没有监管障碍的国家开发的人工智能技术？    提交人    /u/timeforknowledge   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1iiyslz/does_anyone_know_how_or_why_eu_ai_regulations_are/</guid>
      <pubDate>Thu, 06 Feb 2025 09:35:50 GMT</pubDate>
    </item>
    <item>
      <title>人们说‘人工智能不会思考，它只是遵循模式</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1iiygqg/people_say_ai_doesnt_think_it_just_follows/</link>
      <description><![CDATA[但是，如果人类思维不能识别和遵循模式，那它又是什么呢？我们获取现有知识，重新组合，以新的方式应用它——这与人工智能所做的有什么不同？ 如果人工智能可以做出科学发现，发明更好的算法，构建更精确的法律或哲学论点——为什么这不被认为是思考？ 也许唯一的区别是人类感觉他们在思考，而人工智能却没有。如果是这样的话……意识不就是幻觉吗？    提交人    /u/Unique-Ad246   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1iiygqg/people_say_ai_doesnt_think_it_just_follows/</guid>
      <pubDate>Thu, 06 Feb 2025 09:10:54 GMT</pubDate>
    </item>
    <item>
      <title>谷歌母公司 Alphabet 放弃了不将人工智能用于开发武器等用途的承诺。</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1iiekzr/the_google_owner_alphabet_has_dropped_its_promise/</link>
      <description><![CDATA[谷歌母公司 Alphabet 放弃了不将人工智能用于开发武器和监视工具等目的的承诺。 这家美国科技公司周二表示，就在公布低于预期的收益之前，它已经更新了有关人工智能的道德准则，不再提到不追求可能“造成或可能造成整体伤害”的技术。 谷歌人工智能负责人 Demis Hassabis 表示，准则是在不断变化的世界中进行彻底修改的，人工智能应该保护“国家安全”。 在为这一举措辩护的博客文章中，Hassabis 和该公司负责技术和社会的高级副总裁 James Manyika 写道，随着全球对人工智能领导地位的竞争加剧，该公司认为“民主国家应该引领人工智能发展”，并以“自由、平等和尊重人权”为指导。 他们补充说：“我们相信，拥有这些价值观的公司、政府和组织应该共同努力，创造出保护人类、促进全球增长和支持国家安全的人工智能。” 谷歌诞生之初的座右铭是“不作恶”，尽管这后来在 2009 年降级为“口头禅”，并且在 2015 年母公司 Alphabet 成立时并未纳入其道德准则。 人工智能的快速发展引发了关于如何治理这项新技术、如何防范其风险的争论。 英国计算机科学家 Stuart Russell 在 BBC 的 Reith 讲座上发表讲话，警告了开发自主武器系统的危险，并主张建立全球控制系统。 谷歌博客文章称，自该公司于 2018 年首次发布其人工智能原则以来，这项技术已经迅速发展。“数十亿人在日常生活中使用人工智能。人工智能已经成为一种通用技术，也是无数组织和个人用来构建应用程序的平台，”哈萨比斯和曼尼卡写道。 “它已经从实验室里的一个小众研究课题转变为一种像手机和互联网一样普及的技术；它对社会和世界各地的人们有着众多有益的用途，并得到了充满活力的人工智能开发者生态系统的支持。” https://www.theguardian.com/technology/2025/feb/05/google-owner-drops-promise-not-to-use-ai-for-weapons#:~:text=The%20Google%20owner%2C%20Alphabet%2C%20has,developing%20weapons%20and%20surveillance%20tools.    提交人    /u/AravRAndG   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1iiekzr/the_google_owner_alphabet_has_dropped_its_promise/</guid>
      <pubDate>Wed, 05 Feb 2025 16:52:02 GMT</pubDate>
    </item>
    <item>
      <title>每月“是否有一个工具可以……”帖子</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1hr4p1x/monthly_is_there_a_tool_for_post/</link>
      <description><![CDATA[如果您有一个想要使用 AI 的用例，但不知道使用哪种工具，您可以在这里请求社区提供帮助，在此帖子之外，这些问题将被删除。 对于每个回答的人：没有自我宣传，没有参考或跟踪链接。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1hr4p1x/monthly_is_there_a_tool_for_post/</guid>
      <pubDate>Wed, 01 Jan 2025 15:09:07 GMT</pubDate>
    </item>
    </channel>
</rss>